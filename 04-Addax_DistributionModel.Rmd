# Case study 1: Species Occurrence and Habitat Selection

## Introduction

Addax (*Addax nasomaculatus*) are considered one of the rarest antelopes on earth, with an estimated population of <100 individuals in the wild. We assessed the distribution and occurrence of this species from field surveys collected by the [Sahara Conservation Fund](https://www.saharaconservation.org/) over a 7-year period (2008-2014). Our results provide insight into the factors contributing to species occurrence and are guiding field surveys to areas that have the potential to support small and geographically isolated populations. We incorporated field-derived variables of vegetation cover with remote sensing measures of vegetation productivity (NDVI - [Normalized Difference Vegetation Index](https://earthobservatory.nasa.gov/Features/MeasuringVegetation/measuring_vegetation_2.php)) and surface roughness (derived from SRTM - [Shuttle Radar Topography Mission](https://www2.jpl.nasa.gov/srtm/)). Models were fit in a generalized linear regression framework to evaluate and predict addax occurrence.

![Addax moving across shifting sands in the Tin Toumma desert, Niger (Photo: T.Rabeil, SCF)](addax_termit_niger_0512_scf_rabeil_01.jpg)

The following analysis follows steps detailed in:

Stabach, J.A., T. Rabeil, V. Turmine, T. Wacher, T. Mueller, and P. Leimgruber. 2017. On the brink of extinction - Habitat selection of addax and dorcas gazelle across the Tin Toumma desert, Niger. [Diversity and Distributions 23:581-591](./Publication/Stabach_etal_Addax_2017.pdf).

A link to the repository of the analysis is [here](https://github.com/Smithsonian/Addax_DistributionModel.git).

In this case study, we will demonstrate how to combine the skills learned in GIS, remote sensing, and modeling to provide a predictive surface of addax habitat suitability.  This will include:

* Loading and processing available spatial data
* Creating a proxy for surface roughness
* Relating occurrence (*extract*) with remotely sensed data

## Import & Process Spatial Data

One of the most difficult (and perhaps most time consuming) aspects of conducting a spatial analysis is the construction of the spatial database. Main components include locating and downloading available spatial data, mosaicking (if necessary) multiple tiles together, reprojecting the data so that all layers have the same coordinate reference system (CRS), and resampling the cell sizes to match. 

Multiple sources now exist for finding available spatial data, which include [EarthExplorer](http://earthexplorer.usgs.gov/) and [Google Earth Engine](https://earthengine.google.com/).  Some layers, however, can also be directly accessed within [R](https://cran.r-project.org/) using the [raster](https://cran.r-project.org/web/packages/raster/index.html), [Dismo](https://cran.r-project.org/web/packages/dismo/index.html) and [MODIS](https://cran.r-project.org/web/packages/MODIS/index.html) packages (among others).

### Load Packages
Like other exercises in [R](https://cran.r-project.org/), we will first load the necessary packages to perform the analyses. Use `help()` for any operations that you don't understand or that require additional information.  
 
```{r Library1, message=FALSE, warning=FALSE}
# Load required libraries
library(terra)
library(sf)
library(tidyverse)
library(geodata)
```

### Occurrence Data
First, we will load the spatial points dataframe (our sampling point locations). This is the occurrence dataset collected by partners in Niger.  In this case, I have saved the file as a shapefile (Lat/Long, WGS84) that we will load directly into [R](https://cran.r-project.org/).  This file could just as easily be loaded as a `.csv` and then converted to a spatial object by defining the projection.

```{r Spatial Data, warning = F, eval=T}
# Load the point data and project to UTM32N
Addax <- st_read("data/Occurrence_dd.shp")

# Project to UTM32N, WGS84
# See: https://spatialreference.org/
UTM32N.proj <- "epsg:32632"
Addax <- st_transform(Addax, crs = UTM32N.proj) # Note that I am overwriting the original Lat/Long object

# Look at the file
Addax
```

### Raster Data
[R](https://cran.r-project.org/) has a number of options for directly importing remotely sensed data into the console.  Here, we will import a NDVI data layer (a .tif file) that I downloaded from [EarthExplorer](http://earthexplorer.usgs.gov/). This layer is located in your data directory and has already been re-projected to UTM 32N WGS84 and clipped to the study area. We will also use functions in the [raster](https://cran.r-project.org/web/packages/raster/index.html) package to load a 90-m elevation dataset.  Both layers must exactly match each other (same CRS, extent, grid cell size) in order to predict with the rasters.

```{r Raster Data, eval=T}
# NDVI - Normalized Difference Vegetation Index (250-m)
# ********************************************
# ********************************************
# Note the file is already projected to UTM32N WGS84
ndvi <- rast("data/MOD13Q1_Nov2017.tif")
ndvi <- ndvi * 0.0001 # Convert to decimal values (This is done to reduce the file size)
ndvi

# SRTM - Elevation data from the Shuttle Radar Topography Mission (30-m)
# ********************************************
# ********************************************
srtm <- elevation_3s(lon=12, lat=16.5, path = tempdir()) # Inputting general coordinates to grab tile
srtm

# Clip (crop) the raster to the study area extent. This will help with processing the image
# Read in a study area boundary (also in Lat/Long).
SA <- st_read("data/StudyArea.shp")
plot(srtm)
plot(SA, add=T)

# Crop the raster to the study area extent
# You could also manually set the extent, if inclined to do so (SA <- extent(11,13,16,17))
srtm <- crop(srtm,SA)

# Note: In many cases, you might need to download multiple tiles to cover your study area.  To put them together, you'd need to use the mosaic function.  Be careful, rasters can get large.
#srtm1 <- elevation_3s(lon = 12, lat = 16.5, path = tempdir())
#srtm2 <- elevation_3s(lon = 15, lat = 16.5, path = tempdir())
#srtm.mosaic <- mosaic(srtm1,srtm2, fun=mean) # Using mean for overlapping region

# Global Administrative Boundaries (GADM):
# Sometimes very useful to verify you are working where you think you are
Niger <- world(resolution=1, level=0, path = tempdir()) |> st_as_sf() |> filter(NAME_0 == "Niger")
Niger <- st_transform(Niger, crs = 4326)
# Plot together - Overlapping where I think they should!
plot(Niger[1])
plot(srtm, add=T)
```

### Raster Project & Resample

You'll note that the projection of the NDVI layer is different than the SRTM data layer.  We need to project the SRTM data layer to UTM32N WGS, making sure the extents, cell sizes (essential for final prediction), and grids all match.  We'll use bilinear interpolation because the data are continuous.  For a categorical image (e.g., land-cover classification), use nearest neighbor.  We also now have a choice to make about the resolution we will use to conduct our analyses.  What resolution should we use?

```{r Project Raster, warning=FALSE}
# Project the srtm image, setting the resolution
srtm.utm <- project(srtm, "epsg:32632", method = 'bilinear') # This can be a slow process, depending on the size of the raster

# Resample the srtm image to the ndvi image
# The native resolution of our analysis will then be 250m - This will help to speed up the processing, but means we are losing some of the fine-scale information contained in the 90m elevation model which might be important
srtm.250m <- resample(srtm.utm, ndvi, "bilinear")

# Print both rasters and make sure they have the same extent, rows/columns, projection, and resolution
ndvi
srtm.250m
```

**Questions:** 

1) Are these files projected?  How would you check?
2) What is the [R](https://cran.r-project.org/) function for re-projecting raster and/or spatial point data files if you needed to?
3) What should you always do when importing spatial data (also good practice for any data you load into [R](https://cran.r-project.org/))?

```{r Questions, eval=F, echo=F}
# Answers to questions:
# 1) Print the name of the file to the R console.  This will provide information on the attributes of the file.  If projected, these details will appear in the output. Use the function same.crs to compare the projections.
srtm.250m
Addax
same.crs(srtm.250m,Addax)

# 2) For raster and vector files, the function differs
help(project)
help(st_transform)

# 3) Plot your data.  R does not have projection on the fly capabilities, so if your data do not overlay, there is a problem.  See below for plotting of data.
```

Now that we have projected and processed the spatial data, let's make a plot of all the data to make sure everything looks good. Data are located where I expect them to be. Amazing! 

```{r Plot Data, warning=FALSE}
# Plot the result (order matters)
plot(srtm.250m)
plot(Addax,pch=15,cex=0.7,add=TRUE)
```

### Generate Terrain Variables

We assumed that addax would select inter-dunal depressions. Instead of including the raw SRTM (elevation) data in our models, we created a variable derived from the elevation dataset, termed *Surface Roughness*. This variable is the change in elevation range between neighboring pixels. That is, it is the difference between the minimum and maximum values of a cell and its eight surrounding neighbors. Pixels with similar neighborhood values have a low change in elevation range (e.g., flat areas), whereas highly variables neighborhoods have a large change in elevation range (e.g., steep slopes) (Wilson et al. 2007).

Terrain variables can be easily applied to digital elevation models.  Variables that can be calculated include slope, aspect, Terrain Position Index (TPI), Terrain Ruggedness Index (TRI), and surface roughness. Some of these variables are likely to be highly correlated. Use `help(terrain)` for additional information. In this example, we will calculate surface roughness.

```{r Roughness, eval = T, warning=FALSE}
rough <- terrain(srtm.250m, v='roughness')
plot(rough)
plot(Addax,pch=15,cex=0.7,add=TRUE)
```

### Raster Extraction
The `extract` command from the `terra` package is an extremely useful and powerful function for joining spatial objects.  Here, we will extract surface roughness and NDVI at each of the occurrence locations.  If we stack the layers together, we can extract all the rasters together with a single command. See `help(extract)` for additional information about this function. Note that we tell R that we want the function from the `terra` package as it 

```{r Extract, eval=T, warning=FALSE}
# Extract NDVI and rough  
ndvi.point <- terra::extract(ndvi,Addax)
rough.point <- terra::extract(rough,Addax)

# There are some NA, so we will fill them with the mean of all other values. Note this is happening because the ndvi layer we are using here is not the original used in the actual analysis, which matched the Modis images to the time each transect was conducted.
ndvi.point$MOD13Q1_Nov2017 <- ifelse(is.na(ndvi.point$MOD13Q1_Nov2017), mean(ndvi.point$MOD13Q1_Nov2017, na.rm = T),ndvi.point$MOD13Q1_Nov2017)
rough.point$roughness <- ifelse(is.na(rough.point$roughness), mean(rough.point$roughness, na.rm = T),rough.point$roughness)

# Append to spatialobject
Addax <- data.frame(Addax, ndvi=ndvi.point$MOD13Q1_Nov2017,
                    rough=rough.point$roughness) 

# Look at the data
head(Addax)
```

**Note:** We have simplified things a bit here because we have a time series of points and therefore, we must download and extract a time series of NDVI.  Each of the dates need to be aligned with their corresponding NDVI image.  In the next exercise, the dataset provided will already have this step completed so that we can focus on the modeling.

## Fitting a GLM

Now that we have extracted the data, we will now walk through the steps of fitting a logistic regression model to model the probability of addax occurrence. 

We now have the occurrence dataset with all associated variables. 

Animal counts (addax, but also dorcas gazelle) were summarized at plot locations. We re-coded all counts within a 2.5-km radius to a measure of occurrence (i.e., presence/absence). Thus, we modeled the data as a series of 1's and 0's, representing addax occurrence at plot locations. Data were aggregated in this fashion because of variability between surveys (i.e., the transects locations didn't overlap exactly) and because we did not have confidence in the accuracy of the number of individuals recorded at each sighting. In addition, distance to animal sighting locations were only recorded in a subset of the surveys. Sightings \>500-m from the transect were removed due to an assumed undercounting bias (confirmed by investigating the frequency of sightings in relation to distance). This allowed for a conservative broad-scale approach to incorporate extremely messy field data collected over multiple years. See more details in [Stabach et al. 2017](./Publication/Stabach_etal_Addax_2017.pdf).

In this section of the analysis we will:

-   Summarize the occurrence dataset
-   Scale/center parameter Values
-   Assess potential collinearity between variables
-   Apply logistic regression to model the probability of occurrence of Addax
-   Graph response curves and interpret coefficients
-   Validate the result
-   Make a predictive surface

### Load extra packages

As done previously, we remove objects in [R's](https://cran.r-project.org/) memory and load the necessary packages to perform the analyses. Please use the `help()` for any operations that you don't understand or that require additional information. Knowing what to search for in [Google](https://www.google.com/) is essential. I usually start any query with "r-cran" (e.g., "r-cran Import shapefile") to limit the search terms to [R](https://cran.r-project.org/).  **Note:**  There are quite a few additional packages to install ('install.packages("package name")') in this exercise.

```{r Library2, message=FALSE, warning=FALSE}
# Load required libraries
library(lubridate)
library(pROC)
library(tmap)
library(visreg)
library(coefplot)
library(usdm)
```

### Summarize the Occurrence Dataset

As a first step, we will summarize the occurrence dataset and visualize some of the patterns.

### Data Cleaning

Columns in the Addax dataframe include *Date* of survey, *X* and *Y* location of each plot, the number of addax and dorcas gazelle sighted at each location, a unique plot ID (*Pt_ID*), and the presence/absence of vegetation species *Cornulaca monocantha* (Cornul), *Stipagrostis acutiflora* (Stipa1), and *Stipagrostis vulnerans* (Stipa2). These vegetation species were thought *a priori* to influence addax occurence and were collected at each plot location. *Human* disturbance (e.g., footprint, sighting, tire tracks) were also recorded (i.e., *Human* = 1). You'll also note that the remote sensing data that we extracted, surface roughness (*Rough*) and the Normalized Difference Vegetation Index (*NDVI*), are also included.

We need to add a few things to the dataframe, which most importantly includes re-coding the abundance records to a measure of occurrence. This was done because of a lack of confidence in the exact counts collected. Here, we create a `Month`, `Year`, and `Season` field, correct some of the field types, and re-code the occurrence.

```{r Recode}
# Create month, year and season fields
Occ <- Addax %>% st_drop_geometry() %>% mutate(
  Month = month(Date),
  Year = year(Date),
  Season = case_when(
    Month >=3 & Month <= 6 ~ "Dry",
    Month >=7 & Month <=10 ~ "Wet",
    Month >=11 & Month <=12 ~ "Cold",
    TRUE ~ "Fix Problem"
  )
)

# Case_when is the same as:
#Occ$Season <- ifelse(Occ$Month >=3 & Occ$Month <=6, "Dry",
#                          ifelse(Occ$Month >=7 & Occ$Month <=10, "Wet",
#                                 ifelse(Occ$Month >=11 & Occ$Month <=12, "Cold","Fix Problem")
#                          ))

# We could easily include the following in the piping above.
# Recode Occurrence  
Occ$obsAddax <- ifelse(Occ$Addax > 0, 1, 0)
Occ$obsDorcas <- ifelse(Occ$Dorcas > 0, 1, 0)

# Add some summaries
Occ$SumBoth <- Occ$obsAddax + Occ$obsDorcas
Occ$BothPresent <- ifelse(Occ$SumBoth > 1, 1, 0) # Both species present/occurred
Occ$OnePresent <- ifelse(Occ$SumBoth > 0, 1, 0) # At least one species present/occurred

# Correct the data types
# Individually, this would be:
Occ$Cornul <- as.factor(Occ$Cornul)
Occ$Stipa1 <- as.factor(Occ$Stipa1)
Occ$Stipa2 <- as.factor(Occ$Stipa2)
Occ$Season <- as.factor(Occ$Season)
Occ$Year <- as.factor(Occ$Year)
# More succinct

# cols <- c("Cornul", "Stipa1", "Stipa2", "Season", "Year")
# Occ[cols] <- lapply(Occ[cols], as.factor)
# str(Occ)
```

### Data Summary

When we summarize the dataset, we see that the data were collected multiple times a year and that the occurrence of addax (and dorcas gazelle) vary between years and seasons.

```{r Aggregate, warning=F, message=F}
# Summarize
Occ.Summary <- Occ %>%
  group_by(Year, Month, Season) %>%
  summarize(PresAddax = sum(obsAddax),
            PresDorc = sum(obsDorcas),
            PrevAdd= round(sum(obsAddax)/length(obsAddax)*100,digits=1),
            PrevDorc = round(sum(obsDorcas)/length(obsAddax)*100,digits=1),
            One = round(sum(OnePresent)/length(obsAddax)*100,digits=1) ,
            Both = round(sum(BothPresent)/length(obsAddax)*100,digits=1)
            )

# Look at the result
Occ.Summary <- as.data.frame(Occ.Summary)
Occ.Summary

# If you want, write to a file
#write.csv(Occ.Summary, file="Addax_Dorcas_Prevalence.csv", quote = FALSE, row.names = FALSE)
```

Let's look at the data one last time before moving on to the modelling.

```{r Plot}
# Plot
plot(Occ$X,Occ$Y,xlab = "Easting", ylab = "Northing", main = "Plot Locations", frame = FALSE, pch = ".", col="red", cex = 5, asp=1)
```

**Question:**

1)  The field team didn't visit the same plots every year or month. How could you programmatically and efficiently view the plots that were collected each field seasons?

```{r Loop Plot, eval=F, echo=F}
Un.Date <- unique(Occ$YearMonth)

for(i in 1:length(Un.Date)){
  print(Un.Date[i])
  temp.df <- subset(Occ, YearMonth == Un.Date[i])
  # Open png (or jpeg or pdf)
  png(filename = paste0("./Output/",Un.Date[i],".png"))
  plot(temp.df$X,temp.df$Y,xlab = "Easting", ylab = "Northing", main = Un.Date[i], frame = FALSE, pch = ".", col="red", cex = 5, asp=1)
  
  #Close the file
  dev.off()
  
  # Or do it like this
  #plot(Occ[Occ$YearMonth == Un.Date[i],"X"],Occ[Occ$YearMonth == Un.Date[i],"Y"],xlab = "Easting", ylab = "Northing", main = Un.Date[i], frame = FALSE, pch = ".", col="red", cex = 5, asp=1)
}


Addax <- ndvi
ndvi.stack <- stack(ndvi,ndvi)

temp.new <- data.frame()

for(i in 1:length(Date.Un)){
+ temp.df <- subset(Addax, YearMonth == Date.Un[i])
temp.df$ndvi <- extract(ndvi.stack[[i]], temp.df)

temp.new <- rbind(temp.new, temp.df)
}
```

### Scale Continuous Variables

It is often helpful and necessary to scale ($x_i = \bar{x} / sd(x)$) continuous predictor variables that have vastly different value ranges (e.g., elevation and NDVI). See the `scale` function. Doing so can help with model convergence and coefficient comparability. While relationships between your dependent variable and each independent variable will remain the same, it is important to remember that data are not on the same scale as the original values and must be back-transformed when making raster predictions. **This is critical**.

```{r Scale, eval=T, warning=F}
# Scale the continuous variables that we'll include in the modelling
Occ <- Occ %>% mutate(
  sHuman = as.numeric(scale(Human)), # This is a bit ugly, but if you don't specify as "as.numeric", the variables can be difficult to plot
  sndvi = as.numeric(scale(ndvi)),
  srough = as.numeric(scale(rough)),
  sDorcas = as.numeric(scale(Dorcas)),
  sAddax = as.numeric(scale(Addax))
)

# The default scale function simply does the following:
#CenterScale <- function(x){
#  (x-mean(x))/sd(x)
#}

#sndvi2 <- CenterScale(Occ$ndvi)

# These two scaled ndvi vectors should be exactly the same:
#cor(Occ$sndvi,sndvi2)
#summary(Occ$sndvi)
#summary(sndvi2)

# Let's also scale our raster layers by the mean and standard deviation at each location so we don't forget to later when making predictions from our model
s.ndvi <- (ndvi - mean(Occ$ndvi)) / sd(Occ$ndvi)
s.rough <- (rough - mean(Occ$rough)) / sd(Occ$rough)

par(mfrow=c(2,2))
# You should notice that the rasters look exactly the same, but their values have changed.  They are now on a much more similar scale.
plot(ndvi, main = "Non-Scaled NDVI");plot(s.ndvi, main = "Scaled NDVI");plot(rough, main = "Non-Scaled Rough");plot(s.rough, main = "Scaled Rough")
par(mfrow=c(1,1)) # Returning plotting window to normal
```

### Investigate Collinearity

Like any other analysis, we need to evaluate redundancy between our predictor variables to make sure they are sufficiently independent. Otherwise, we may obtain be unable to interpret our model. We will conduct a Variance Inflation Factor Analysis (VIF), which seems to be the preferred statistical method for comparing collinearity currently. Any variables with a VIF \> 3 are potential cause for concern. Are any of the continuous variables included in our analysis highly collinear?

**Question:**

1)  What do we do with categorical variables?

```{r Cat Collinearity, eval=F, echo=F}
# Not much really.  We can't include this categorical variables in collinearity analyses.
```

```{r Corr, eval=T}
# Assess the continuous variables we'll include in our analysis
vifstep(Occ[,24:28]) # Including scaled Human Presence, NDVI, roughness, and Dorcas/Addax

# You could also use the cor function to investigate correlation.
# What is a reasonable correlation threshold to use |0.65|??
#cor(Occ[,22:26])
```

### Generalized Linear Regression (GLM)

Model the occurrence of addax in a Generalized Linear Regression (GLM) framework. We expected non-linear relationships with surface roughness and ndvi, so are including these terms as quadratic effects in the model. Our goal *here* was not necessarily to create the very best model. Instead, we aimed to:

1.  Identify the relationships between **all** reasonable predictor variables and addax occurrence
2.  Evaluate a sub-model that contains only the remote sensing layers to make a prediction of habitat suitability that can be extrapolated across the landscape

**Question:**

1)  Why is using a GLM advantageous given the data and objectives?

```{r Model, eval=T}
# Create a full model with all the variables you think are important predictors of addax occurrence
glm.Addax <- glm(obsAddax ~ srough + I(srough^2) + sndvi + I(sndvi^2) + sHuman + sDorcas + Stipa1 + Stipa2 + Cornul + Season + Year, 
                 data = Occ, 
                 family = binomial(link="logit"))
# Summarize result
summary(glm.Addax)

# Nice summary table, including model estimates and odds ratios
#tab_model(glm.Addax)

# Or print the confidence intervals
#confint(glm.Addax)
```

### Graph and Interpret

Use the `visreg` and `coefplot` functions to easily graph the results from a glm and evaluate the model output. Be careful, however, with how categorical variables are interpreted. `Visreg` does the hard parts of providing a predictive response while holding all other variables constant.

```{r Model Graph, eval=T}
# Plot the coefficients
coefplot(glm.Addax,
        plot=TRUE,
        mar=c(1,4,5.1,2),
        intercept=FALSE,
        vertical=TRUE,
        main="",
        var.las=1,
        frame.plot=FALSE)

# Reset plotting
par(mar=c(5,4,4,2)+0.1) 

# Graph result for all variables
#visreg(glm.Addax, scale="response", ylab="Prob", partial=TRUE)

# Or just one variable at a time
par(mfrow=c(1,2))
visreg(glm.Addax,"srough",
       scale="response", 
       ylab="Probability of Occurrence", 
       xlab="Surface Roughness",
       partial=TRUE, 
       line=list(col="blue"), 
       fill=list(col="gray"),
       points=list(col="black",cex=0.25,pch=19),
       ylim=c(0,1))
visreg(glm.Addax,"sndvi",
       scale="response", 
       ylab="Probability of Occurrence",
       xlab="NDVI",
       partial=TRUE, 
       line=list(col="blue"), 
       fill=list(col="gray"),
       points=list(col="black",cex=0.25,pch=19),
       ylim=c(0,1))
par(mfrow=c(1,1))
```

### Model Validation

Binomial data can be notoriously difficult to validate, at least when compared to standard tools used for linear regression. Ramiro discussed a few techniques to assess model assumptions, including ways to assess a model's predictive power. We will calculate the Area Under the Curve (AUC) to assess predictive power here. AUC compares the difference between the true positive classification rate and a false positive rate (i.e., Specificity vs Sensitivity). 

Another (best) option is to incorporate an independent dataset for validation. 

Some guidelines for AUC:

-   0.9 - 1: Excellent (A)
-   0.8 - 0.9: Good (B)
-   0.7 - 0.8: Fair (C)
-   0.6 - 0.7: Poor (D)
-   0.5 - 0.6: Fail (F)

```{r Validation,eval=T}
# Evaluate deviance residuals
# No strong evidence of lack of fit.  Most residuals are around a value of 0.
devresid <- resid(glm.Addax, type = "deviance")
hist(devresid)

# Calculate AUC
predpr <- predict(glm.Addax, type=c("response"))
(roccurve <- roc(Occ$obsAddax ~ predpr))
plot(roccurve, main="AUC")
```

### Make a Predictive Surface

One of the most valuable parts of a species distribution model is predicting to locations where surveys were not performed. In order to make a prediction at these locations, we need predictor data that has wall-to-wall coverage. Unfortunately, only two of our layers incorporated in the full model have full coverage (*NDVI* and *Surface Roughness*).

Create a model with these two layers, assess how the model compares with the full model and predict across the entire study area. As you will see, model statistics indicate that this sub-model is not as good as the full model (compare the AIC, AUC).

The model, however, can still be useful, as long as we are clear about its shortcomings (e.g., we'd expect the predictive power to be decreased since we are not including the fine scale data collected at individual plot locations).

### Create Model Subset

```{r ModelSub, eval=T}
glm.Addax2 <- glm(obsAddax ~ srough + I(srough^2) + sndvi + I(sndvi^2), 
                  data = Occ, 
                  family = binomial(link="logit"))

# Summarize and print confidence intervals
summary(glm.Addax2)

# View model output with odds ratios
#tab_model(glm.Addax2)

# Calculate AUC
predpr <- predict(glm.Addax2, type=c("response"))
(roccurve <- roc(Occ$obsAddax ~ predpr))
plot(roccurve) # Not great
```

**Question:**

1)  Is our reduced model as good as our full model? 

### Raster Prediction

We then can use the predict command to take our coefficients and predict

```{r Model Predict, eval=T}
# We could physically calculate the prediction from the model coefficients:
#coef <- summary(glm.Addax2)
#coef <- coef$coefficients
#coef

#Addax.predict <- (exp(coef[1] + s.rough*coef[2] + s.rough^2*coef[3] + s.ndvi*coef[4] + s.ndvi^2*coef[5])/(1 + exp(coef[1] + s.rough*coef[2] + s.rough^2*coef[3] + s.ndvi*coef[4] + s.ndvi^2*coef[5])))

# Add to a stack of rasters and rename layer names.
satImage <- c(s.rough, s.ndvi)
names(satImage) <- c("srough", "sndvi")

# Predict and export image to directory
Addax.predict <- predict(satImage, glm.Addax2, type="response", progress='text')

# Plot result
plot(Addax.predict) # Not a great prediction, with mostly low values, but it highlights some important aeras.
```

### Interactive Mapping

Lastly, we can take this one step further and plot our predicted surface on an interactive map, so that we have a better idea of its context in the real world and also provide a way to post our results on relevant social media.

```{r Interactive, eval=T, warning=F, message=F}
# Load ESRI imagery baselayer
tmap_mode("view")
tm_basemap("Esri.WorldImagery") +
  tm_shape(Addax.predict, name = "Addax prediction") +
  tm_raster(palette="-inferno", n=8, alpha=0.6, 
            title = "Predicted Addax Occurrence")
```
